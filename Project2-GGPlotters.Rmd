---
title: "Project2-GGPlotters"
author: "The GGPlotters - Chirag Kulkarni, Lidia Solorzano, Wendy Huang, Nojan Sheybani"
date: "11/13/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

First, we must import all of the libraries that we may use and read in our data.
```{r}
library(rpart)
library(C50)
library(gmodels)
library(tidyverse)
library(cvTools)
library(caret)
library(lubridate)
library(adabag)

#install.packages("lubridate")

test <- read.csv("~/Desktop/College/Seventh Semester/Data Science/DS-Project-2/sales_manhattan_test_set.csv")
train <- read.csv("~/Desktop/College/Seventh Semester/Data Science/DS-Project-2/sales_manhattan_train_set.csv")
```


Just as a sanity check, we look at the summary of train to see if we've correctly imported the data. This also allows us to look at the data and start brainstorming which features we can use in our predictive models and which features we can engineer.
```{r}
summary(train)
```

We saw that there were 6007 0's in gross square feet, so we decided that we should take these out before moving forward. We figured that gross square feet could play a huge role in the value of a house (whether or not it is "high end"). We realized that a major problem we could hit once training our model with the current values of gross square feet is that we could be introduced a new square footage in the prediction phase. In order to make gross square feet more general, we decided to change the values in the column to ranges, as can be seen below. This ensures that any value that is introduced will fall into some range.
```{r}
train_edit <- train
test_edit <- test
#converting GSF to characters to turn into numeric
train_edit$GROSS.SQUARE.FEET <- as.character(train_edit$GROSS.SQUARE.FEET)
#turning GSF into numeric and taking out commas so no NA's are introduced
train_edit$GROSS.SQUARE.FEET <- as.numeric(gsub(",","",train_edit$GROSS.SQUARE.FEET))
#doing the same for the training set
test_edit$GROSS.SQUARE.FEET <- as.character(test_edit$GROSS.SQUARE.FEET)
test_edit$GROSS.SQUARE.FEET <-as.numeric(gsub(",","",test_edit$GROSS.SQUARE.FEET))        
#taking out missing values
train_edit1 <- filter(train_edit, GROSS.SQUARE.FEET != 0)
train_edit2 <-train_edit1
#mutating based on a range of values
#i looked at histograms and density curves to see what ranges make sense
#more granular for lower values and then get more general
train_edit2 <- train_edit2 %>% mutate(GROSS.SQUARE.FEET = ifelse(GROSS.SQUARE.FEET < 600, "<600", 
                                                          ifelse(GROSS.SQUARE.FEET < 1000, "600-1000",
                                                          ifelse(GROSS.SQUARE.FEET < 1500, "1000-1500",
                                                          ifelse(GROSS.SQUARE.FEET < 2000, "1500-2000",
                                                          ifelse(GROSS.SQUARE.FEET < 2500, "2000-2500",
                                                          ifelse(GROSS.SQUARE.FEET < 3000, "2500-3000",
                                                          ifelse(GROSS.SQUARE.FEET < 4000, "3000-4000",
                                                          ifelse(GROSS.SQUARE.FEET < 5000, "4000-5000",
                                                          ifelse(GROSS.SQUARE.FEET < 10000, "5000-10000",
                                                          ifelse(GROSS.SQUARE.FEET < 20000, "10000-20000",
                                                          ifelse(GROSS.SQUARE.FEET < 30000, "20000-30000",
                                                          ">30000"))))))))))))

#turning it into a categorical variable
train_edit2$GROSS.SQUARE.FEET <- as.factor(train_edit2$GROSS.SQUARE.FEET)
#doing the same for the test set
test_edit2 <- test_edit %>% mutate(GROSS.SQUARE.FEET = ifelse(GROSS.SQUARE.FEET < 600, "<600", 
                                                        ifelse(GROSS.SQUARE.FEET < 1000, "600-1000",
                                                        ifelse(GROSS.SQUARE.FEET < 1500, "1000-1500",
                                                        ifelse(GROSS.SQUARE.FEET < 2000, "1500-2000",
                                                        ifelse(GROSS.SQUARE.FEET < 2500, "2000-2500",
                                                        ifelse(GROSS.SQUARE.FEET < 3000, "2500-3000",
                                                        ifelse(GROSS.SQUARE.FEET < 4000, "3000-4000",
                                                        ifelse(GROSS.SQUARE.FEET < 5000, "4000-5000",
                                                        ifelse(GROSS.SQUARE.FEET < 10000, "5000-10000",
                                                        ifelse(GROSS.SQUARE.FEET < 20000, "10000-20000",
                                                        ifelse(GROSS.SQUARE.FEET < 30000, "20000-30000",
                                                        ">30000"))))))))))))

test_edit2$GROSS.SQUARE.FEET <- as.factor(test_edit2$GROSS.SQUARE.FEET)

train_with_gsf <- train_edit2
test_with_gsf <- test_edit2
```

Now that we have re-engineering gross square feet, we can start making some new columns. In order to take out any rows that we would not want to use, we start off with taking out all of the rows that have a 0 in the year built column. Then we start engineering some new features. We start by making month and year bought columns from the sale date column. This provides a more general way to compare the prices of a house based on month and year bought. We then use the newly generated month bought column to create a season column in order to take the season of purchase into account. Finally, we make an age column, which uses the newly generated year bought column and existing year built column to calculate how old the building is at time of purchase. Obviously, property often loses value as it ages and newer properties are more valuable.
```{r}
filtered_train <-  train_with_gsf %>% filter(YEAR.BUILT!=0)

train_with_month <- filtered_train %>% mutate("MONTH.BOUGHT"=month(mdy(SALE.DATE)))
test_with_month <- test_with_gsf %>% mutate("MONTH.BOUGHT"=month(mdy(SALE.DATE)))

train_with_year <- train_with_month %>% mutate("YEAR.BOUGHT"=year(mdy(SALE.DATE)))
test_with_year <- test_with_month %>% mutate("YEAR.BOUGHT"=year(mdy(SALE.DATE)))

train_with_season <- train_with_year %>% mutate("SEASON" = ifelse(MONTH.BOUGHT %in% c(12,1,2), "Winter",
                                                           ifelse(MONTH.BOUGHT %in% c(3,4,5), "Spring",
                                                           ifelse(MONTH.BOUGHT %in% c(6,7,8), "Summer",
                                                           ifelse(MONTH.BOUGHT %in% c(9,10,11), "Fall",
                                                           "No Season")))))

test_with_season <- test_with_year %>% mutate("SEASON" = ifelse(MONTH.BOUGHT %in% c(12,1,2), "Winter",
                                                         ifelse(MONTH.BOUGHT %in% c(3,4,5), "Spring",
                                                         ifelse(MONTH.BOUGHT %in% c(6,7,8), "Summer",
                                                         ifelse(MONTH.BOUGHT %in% c(9,10,11), "Fall",
                                                         "No Season")))))

train_with_age <- train_with_season %>% mutate("AGE"=YEAR.BOUGHT - YEAR.BUILT)
test_with_age <- test_with_season %>% mutate("AGE"=YEAR.BOUGHT - YEAR.BUILT)

```

After feature engineering, we can start narrowing down the columns we will train our model with. We use columns that we engineered in the model, and the rest of the columns chosen were based on us reading the data dictionary and deciding whether or not that could play a role in the value of a property.
```{r}
cat_vector <- c("NEIGHBORHOOD", "BUILDING.CLASS.CATEGORY", "TAX.CLASS.AT.PRESENT", "BLOCK", "TAX.CLASS.AT.TIME.OF.SALE", "ZIP.CODE", "GROSS.SQUARE.FEET", "MONTH.BOUGHT", "LOT","AGE", "SEASON")
train_selected <- select(train_with_age,cat_vector, "high.end")
test_selected <- select(test_with_age,cat_vector)
```

Having our training and testing sets ready, we can start building and testing a model. Our first approach was using cross-validation with decision trees. In this block, we run the cross validation algorithm to find the optimal depth and the highest accuracy that can be used with our model. After doing so, we can use the optimal depth to build a new model that we test our testing set with to build our submission to the competition.
```{r, eval=F, echo=T}
afdsfa
## Create k folds 
k=6
folds <- cvFolds(nrow(train_selected), K = k)

## Create a vector called accuracy.vector that will store 
accuracy.vector <- matrix(data = NA, nrow = k, ncol = 1)
## Create a vector to store optimal tree depth for each CV iteration

opt.depth <- matrix(data = NA, nrow = k, ncol = 1)

## Do this for loop for each fold
for(i in 1:k){
  ## Make everything else that isn't fold i the training set 
  trainFold <-train_selected[folds$subsets[folds$which != i], ] 
  
  ## Make fold i the validation set
  valid <- train_selected[folds$subsets[folds$which == i], ]
  
  ## Use the training set to create a model for prediction
  y<-as.factor(trainFold[,ncol(trainFold)])
  x<-trainFold[,1:(ncol(trainFold)-1)]
  
  y_valid<-as.factor(valid[,ncol(valid)])
  x_valid<-valid[,1:(ncol(valid)-1)]
  
  errors<-matrix(data=NA,nrow=15,ncol=3)
  
  for(dep in 1:15){
    errors[dep,1]<-dep
    fit<-rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1,maxcompete=0,
                                           xval=0,maxdepth=dep,)) 
    
    errors[dep,2]<-sum(y!=predict(fit,x,type="class"))/length(y)
    errors[dep,3]<-sum(y_valid!=predict(fit,x_valid,
                                       type="class"))/length(y_valid)
    
  }
  ## What is the min value in the accuracy sequence
  min.value <- min(errors[,3])
  
  ## Find the depth values that maximaze the accuracy sequence
  optimal.depth.values <- c()
  for(dep in 1:length(errors[,3])) {
    if(errors[dep, 3] == min.value) {
      optimal.depth.values <- c(optimal.depth.values, errors[dep, 1])
    }
  }
  
  ## If there is only one value in the list, then set maxdepth equal to that value
  ## If there is more than one value in the list, then set the maxdepth equal to the 
  ## middle value of the list that minimizes the error on the validation set 
  if(length(optimal.depth.values == 1)) {
    maxdepth <- optimal.depth.values[1]
  } 
  else {
    if((length(optimal.depth.values) %% 2) == 0) {
      maxdepth <- optimal.depth.values[length(optimal.depth.values)/2]
    } 
    else {
      maxdepth <- optimal.depth.values[ceiling(length(optimal.depth.values))]
    }
  }
  ## Store the accuracy when fold i is used as the validation set into the ith index of the vector
  opt.depth[i] = maxdepth
  accuracy.vector[i] = 1-errors[maxdepth, 3]
}
```

The most frequent optimal depth in our opt.depth vector was 14, so we decided to use that as the input for the maxdepth parameter of rpart.control. After using this algorithm and submitting to Kaggle many times, we were able to achieve a max accuracy of ~80.2, so we decided to pursue other algorithms in order to increase our
```{r, eval=F, echo=T}
y<-as.factor(train_selected[,ncol(train_selected)])
x<-train_selected[,1:(ncol(train_selected) - 1)]

x_valid<-test_selected[,1:(ncol(test_selected))]
  
fit<-rpart(y~.,x,control=rpart.control(minsplit=0,minbucket=0,cp=-1,maxcompete=0,
                                           xval=0,maxdepth=14,)) 

predictions <- as.data.frame(predict(fit,x_valid,type="class"))
rbind(predictions, c("ID, high.end"))
write.csv(predictions, "~/Desktop/College/Seventh Semester/Data Science/DS-Project-2/prediction.csv")
dim(test_selected)

```

In this block, we attempt to use the random forest technique. The random forest program in R runs into errors when there are more than 53 categories in a column, so we first had to get around that. Upon that workaround, we were able to run the random forest algorithm. This took FOREVER and resulted in a lower accuracy than we achieved with the cross-validation technique, so we decided to move on to a different technique.
```{r, eval=F, echo=T}
train_factor <- train_selected
                                        
train_factor$high.end <- as.factor(train_factor$high.end)
n <- names(train_factor)
f <- rep(NA, times = 12 )
for (i in 1:12){
  feature <- train_factor[,i]
  num <- n_distinct(feature)
  if(num <= 53){
    f[i] <- n[i]
  }
}
f<- f[which(!is.na(f))]
train2 <- select(train_factor, f)


#trying a random forest
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=0)
#Metric compare model is Accuracy
metric <- "Accuracy"

## Let's perform a "grid search" to determine the best value of "mytry."  Be prepared to wait . . .
set.seed(42)
#tunegrid <- expand.grid(.mtry = c(1:8) -- we optimized .mtry and found that 8 yielded the best results

tunegrid <- expand.grid(.mtry = c(1:9))
rf_default <- train(high.end~., data = train2, method = 'rf', metric = metric, tuneGrid = tunegrid, 
                    trControl = control)
print(rf_default)
predictions2 <- as.data.frame(predict(rf_default, x_valid, type = "prob"))
predictions2
summary(predictions2)

write.csv(predictions,"~/Desktop/ggplotters/DS-Project-2/predictions.csv")
```

In this block, we attempt to optimize the random forest algorithm for n-tree, but it ended up crashing R.
```{r, eval=F, echo=T}
#optimizing for n-tree, this did not work. R crashed
modellist <- list()
for (ntree in c(1000, 1500, 2000, 2500)) {
  set.seed(123)
  fit <- train(high.end~., data=train2, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control, ntree=ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit
}
# compare results
results <- resamples(modellist)
summary(results)
dotplot(results)
```

The next approach we decided to utilize was bagging. We used a large number of bags due to our large dataset.
```{r, eval=F, echo=T}
#install.packages('ipred')
library(ipred)
bagging <- bagging(high.end~., data = train_selected, coob = TRUE, nbagg=500)
bagging
```

When we submitted to Kaggle, we saw an increase in accuracy to about 81.3%.
```{r, eval=F, echo=T}
x_valid<-test_selected[,1:(ncol(test_selected))] # The test set 
  
bagging_p <- as.data.frame(predict(bagging,x_valid,type="class"))  
rbind(bagging_p, c("ID, high.end"))
write.csv(bagging_p, "prediction.csv")
dim(test_selected)
```

Next, we decided to use the Adaboost algorithm to try to improve accuracy. Instead of doing any partitioning like we did in class, we just used the whole training set in the adaboost algorithm. We also used a much higher number of trees, because we had much more data. We use errorevol to track how our accuracy is changing as we change different parameters. 
```{r}
set.seed(12345)
ind <- createDataPartition(train_selected$high.end, p= 1, list = FALSE)

# split the data frames
bc_train <- train_selected[ind, ]
bc_test  <- train_selected[-ind, ]

bc_train$high.end <- as.factor(bc_train$high.end)
## Now let's give boosting a try, using the "adabag" package in R.
adaboost <- boosting(high.end ~ ., data=bc_train, boos = FALSE, mfinal=50)

errorevol(adaboost, bc_train)
#Grid <- expand.grid(maxdepth=c(1,2,3,4,5,6,7),nu=.01,iter=c(50,100,150,200))
#results_ada = train(high.end~., data=bc_train, method="ada",
#                    trControl=cv_opts,tuneGrid=Grid)
```

Now that we've stored our model in the adaboost variable, we can run our testing set through it. After this, we build a Kaggle submission in a csv file. This technique resulted in an accuracy of 82.5.
```{r}
p <- predict(adaboost,test_selected)
predictions <- as.data.frame(p$class)
write.csv(predictions, "~/Desktop/College/Seventh Semester/Data Science/DS-Project-2/prediction.csv")
```